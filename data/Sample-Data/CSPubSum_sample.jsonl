{"conclusion": "The KEGG website provides a wealth of information relating genomes to life and the environment, and is therefore a critical resource for research in the life sciences field. However, the pathway maps in the KEGG PATHWAY database are static. That is, they can only be viewed; not interactively explored. As a consequence, they are of only limited use for education and learning purposes. Accordingly, this study has proposed a system in which the static pathway maps in KEGG are converted into interactive maps such that they can be explored at will. The proposed system supports two specific functions, namely (1) the automatic identification of all possible reaction paths between two nominated genes; and (2) the automatic identification of all similar reactions and reaction paths in two different pathways. In general, the results of a preliminary trial have suggested that the system provides a versatile tool for life sciences education purposes. However, further work is required to improve the visualization interface; particularly for large-scale, complex pathway maps.In the proposed system, the pathway maps are generated directly from the KGML data. Thus, the accuracy of the maps is fundamentally dependent on the accuracy of the corresponding KGML file. In practice, the KGML file may contain errors or omissions, and hence the machine-drawn pathway maps may differ slightly from the corresponding hand-drawn maps. Accordingly, in a future study, an artificial intelligence mechanism will be developed to visualize a wider range of gene annotations and to detect errors and omissions in the KGML file in order to improve the accuracy and completeness of the machine-generated pathway maps.", "paper_id": "S0933365714000827", "author-highlights": "The study proposes a web service system for exploring pathways in an interactive manner.Pathways can be converted from a static format into an interactive format.The entries in one pathway can be moving or rotating, and multiple pathways can be displayed simultaneously without overlaps.The system can discovery all possible reaction paths between two selected genes and chosen reaction types.Automatic identification of common entries and similar reaction paths in two different pathways.", "main-title": "Interactive web service system for exploration of biological pathways", "summary": "Objective", "abstract": "Objective", "introduction": "Due to rapid advances in the biotechnology and systems biology fields, a huge amount of experimental data is now available regarding the interactions between the components of biological systems and the manner in which these interactions determine the function and behavior of the system. Many bioinformatics resources have been developed to store this information and to facilitate its sharing amongst national and international bodies. Amongst such resources, the Kyoto Encyclopedia of Genes and Genomes (KEGG), developed jointly by the Bioinformatics Center of Kyoto University and the Human Genome Center of the University of Tokyo, is one of the most commonly used in academic and commercial circles [1\u20133]. KEGG provides a wealth of information linking genes and genomes to life and the environment. The database is used to understand high level functional meanings and utilities of the cell or the organism from molecular information [4], and is one of the most commonly used bioinformatics resources in the life sciences research field.KEGG comprises multiple databases relating to systems information, genomic information and chemical information, respectively. The genomic and chemical information databases contain the molecular building blocks of life in the genomic and chemical spaces, respectively, while the systems information database contains the functional aspects of the biological systems constructed using these building blocks. KEGG also contains a PATHWAY database comprising a collection of manually drawn maps describing the molecular interaction and reaction networks pertinent to metabolism (and other cellular processes) and human diseases. Importantly, the pathway maps are converted into a KEGG Markup Language (KGML) format in order to facilitate their exchange over the Internet and to support automatic drawing, computational analysis and modeling applications. The KGML files for metabolic pathway maps contain two different types of graph object pattern showing how enzymes and compounds are linked by \u201crelations\u201d and \u201creactions\u201d, respectively. By contrast, the KGML files for non-metabolic pathway maps show only how proteins are linked by \u201crelations\u201d.However, the databases within KEGG generally lack a user-friendly, interactive interface. Furthermore, the maps within the PATHWAY database are simply manually drawn representations of published information, and are static in the sense that they present a single, fixed view of the pathway. As a result, they do not easily support such functions as determining all of the possible reaction paths between two genes (or compounds) or comparing two different pathways in order to identify any common reaction sequences between them.Accordingly, the present study proposes an intuitive and interactive web service system for interrogating the contents of the KEGG PATHWAY database. In the proposed system, the pathways requested by the user are downloaded from the KEGG database and converted into an interactive format such that an exhaustive search can be made for all of the possible reaction paths between a given start gene (or compound) and a given end gene (or compound). In addition, the system provides the means to view two pathways simultaneously and to identify any reaction sequences which are common to both pathways. Significantly, the user can explore the displayed pathway(s) interactively by clicking on the genes or compounds of interest in order to reveal the related annotations, define the start and end points of the reaction sequence search process, and so on. Moreover, in contrast to the conventional KEGG pathway maps, the maps generated by the proposed system have a three-dimensional (3D) format and are therefore more easily visualized by the user. The feasibility of the proposed system is investigated by means of a pilot study involving 10 students with varying degrees of experience of the KEGG website and its operations.The remainder of this paper is organized as follows. Section 2 reviews the related studies. Section 3 presents an overview of the proposed system and describes its detailed functions and algorithms. Section 4 shows some typical results obtained from the proposed system. Section 5 presents and discusses the feedback received from the preliminary user test of the proposed system. Finally, Section 6 summarizes the major contributions of the present study and indicates the intended direction of future research.Pathway diagrams provide a visual representation of a reaction network help biochemists understand the complex relationships which exist among different reactions and reaction networks. The KEGG system provides a vast repository of information relating to genomes, enzymatic pathways and biological chemicals. This information is stored within four main databases, namely GENES, LIGAND, PATHWAY and BRITE. GENES comprises a collection of gene catalogs for all complete genomes and some partial genomes generated from publicly available resources. LIGAND contains a bank of knowledge pertaining to the universe of chemical substances and reactions that are relevant to life, and is organized in the form of three sub-databases, namely COMPOUND, GLYCAN and REACTION. Meanwhile, the PATHWAY database contains a collection of manually drawn pathway maps representing existing knowledge regarding the molecular interaction and reaction networks associated with Metabolism, Genetic Information Processing, Environmental Information Processing, Cellular Processes, and Human Diseases. Finally, BRITE comprises a collection of hierarchical classifications representing contemporary knowledge relating to various aspects of biological systems, such as Protein families, organ systems and Disease markers. Among the four databases, GENES and PATHWAY are the most commonly used to search for gene sequences and functional structures in metabolic pathways.Much work has been done concerning the visualization and rebuilding of metabolic pathways [5,6]. Okuda etal [7] proposed a graphical interface for the PATHWAY and BRITE databases designated as KEGG Atlas. The proposed interface consists of a global map and an associated viewer covering approximately 120 KEGG metabolic pathway maps and 10 BRITE hierarchies, and provides the user with the means to map high-throughput experimental data for different organisms to the global map. Karp etal proposed a graphical user interface based on a dynamic query process for the automated sketching of metabolic pathways [8,9].Various methods have been proposed for extracting the metabolic pathways in KEGG. Schreiber [10] classified the methods used to represent the pathway information as either static visualization methods or dynamic visualization methods. In static methods, the pathway diagrams generally take the form of simple pictures and figures reproduced from medical textbooks, teaching materials, information released by research bodies, and so forth. However, such diagrams only represent the best knowledge available at the time of drawing and inevitably become outdated as new knowledge emerges. By contrast, dynamic visualization methods construct pathway diagrams on demand, and draw on the latest information available within the underlying data source when doing so. Typically, dynamic visualization methods are based on a graph drawing approach, in which the nodes represent the substance and enzymes, while the edges represent the reactions. Schreiber surveyed several graph drawing methods and then presented a constraint-based graph drawing algorithm for the construction and comparison of metabolic pathways in different species [11,12]. In the proposed system, the nodes belonging to different sub-graphs were replaced with a single node on the same horizontal layer if they pertained to the same substance, thereby constraining identical reactions to appear in the same layer of the pathway representation.Tsay etal [13] observed that many complex pathways actually comprise a hierarchical structure and can be partitioned recursively into several sub-pathways. Accordingly, they proposed an algorithm designated as the Hierarchically Organized Layout (HOLY) algorithm comprising three main stages, namely Decomposition, Layout and Layout-joining. The Decomposition stage comprises two phases, ie, a Grouping phase and a Partitioning phase. The Grouping phase identifies overlapped groups and acyclic groups, respectively. The former groups are sub-pathways which are highly overlapped, and therefore contain many components which are highly related. The group will be represented as a single node. Meanwhile, the latter groups are used to decompose global cycles into metabolic processes. Having identified the various groups, the Partitioning phase separates them into strongly related components and produces a tree of the group components. The Layout stage in the HOLY algorithm recognizes edges for connected components and the direction of acyclic components. The last stage, the Layout-joining, is used to join the sub-pathways and reduce number of crossing edges. The stage first recognizes the center of each component and iteratively joins a new component to the main component by means of an attaching point. It then fixes the attaching point and rotates the new component in such a way as to maximize the separation distance between its center point and that of the main component.Wolf etal [14] developed a color-based visualization tool in which gene/EC numbers and expression data were imported from KEGG and used to construct colored maps comprising blocks rendered in accordance with the corresponding expression level, as determined using a predetermined color scale. Becker and Rojas [15] proposed an algorithm, for drawing biochemical networks which first checked for the existence (or otherwise) of cycles in the reaction paths, and then adopted different strategies to draw the corresponding network. The drawing process was performed in accordance with the constraints that the nodes in the network must not overlap and nodes belonging to a different cycle should be positioned close to the nodes associated with the cycle to which they are connected. To achieve these constraints, each edge was assumed to act as a spring with a preferred length and to exert a repulsive or attractive force on the connected nodes. The total energy of the network was then minimized by allowing the nodes to move iteratively in the direction of the exerted forces. Kamburov etal [17] proposed the ConsensusPathDB which integrates multiple biology database resources. The system employs statistic tests to interpret the transcriptomics and proteomics data and uses overlap graphs to visualize functional gene sets. The system also provides the function of induced network modules analysis to visualize possible interconnections of a given set of seed genes."}
{"conclusion": "The paper presents a physically based approach to model vehicle dynamics, transient engine performance and engine thermal management systems. The approach enables modeling dynamic processes in the individual components and the dynamic interaction of all relevant domains. The paper focuses on the integration of a very detailed lubrication and split cooling circuit into a modeling framework for simulating vehicle dynamics and transient engine performance. The applied modeling depth is of particular importance for enabling physically plausible distribution of heat fluxes from the combustion chamber walls to both branches of the split cooling system. It also enables modeling back influences of the ETM circuits to in-cylinder phenomena. In addition, an overall coupled approach also brings benefits is terms of model-setup and model manipulation. To ensure short computational times innovative solver techniques suited to account for characteristic time scales of individual domains are introduced. This allows for very short computational times of the overall vehicle, engine and thermal network model, which amount to approximately 1.7\u20131.8 real-time, ie physical time being simulated, for different drive cycles during office simulation on a single core of a PC.The simulation framework is used to evaluate the impact of the different approaches for propelling the coolant pump during two driving cycles featuring significantly different velocity profiles. Analyses were performed using a validated system level model of a passenger car. It was shown that for both drive cycles the fuel saving due to application of an EWP is relatively small and amounts between 0.75% and 1.1%. These results correspond to the lower end of the fuel savings figures published by other researches. This can be explained by the fact that a very modern engine with highly optimized cooling network was selected as the baseline engine. It is shown in the paper that the EWP configuration allows for shorter warm up times of the coolant, which results in slightly higher average combustion chamber wall temperatures and thus in marginally higher indicated efficiency. Higher temperatures of the combustion chamber walls, of the exhaust ports and of the exhaust manifold more importantly increase the exhaust gas temperature during the warm up period. This favors faster light-off of the catalyst. In addition, it was shown that the EWP configuration also allows for higher coolant temperatures as it enables nearly instantaneous increase of the coolant mass flow irrespective of the engine speed. This allows for higher oil temperatures, which results in slight reduced frictional losses after the engine warm-up period. One of the main contributions in reducing fuel consumption of the EWP configuration is the reduced energy needed to power the coolant pump. This mechanism is particularly emphasized for drive cycles with very low average engine load as the need for heat rejection is very small for such cycles. Here, it needs to be considered that the fuel economy is enhanced through this mechanism only if lower power consumption of the EWP is able to at least compensate for losses associated with mechanic-electric-mechanic conversion of the energy needed for powering the EWP.", "paper_id": "S0965997814000337", "author-highlights": "A physically based approach to model all relevant domains of the vehicle is presented.Innovative solver framework considers characteristic time scales of different domains.Time domain decoupling results in very short computational times of the overall model.An electric coolant pump allows for minor fuel savings of the analyzed passenger car.An electric coolant pump contributes to faster engine warm-up and catalyst light-off.", "main-title": "Assessment of engine thermal management through advanced system engineering modeling", "summary": "A physically based approach to model vehicle dynamics, transient engine performance and engine thermal management system is presented. This approach enables modeling dynamic processes in the individual components and is the dynamic interaction of all relevant domains. The modeling framework is based on a common innovative solver, where all processes are solved using tailored numerical techniques suited to account for characteristic time scales of individual domains. This approach enables achieving very short computational times of the overall model. The paper focuses on the integration of cooling and lubrication models into the framework of a vehicle dynamics simulation including transient engine performance demonstrated on a modern passenger car featuring split cooling functionality. A validated model with a mechanically driven coolant pump provides the base for analyzing the impact of introducing an electrically driven coolant pump. Analyses are performed for two drive cycles featuring significantly different velocity profiles to reveal their influences on the operational principles of the powertrain components and their interaction. The results show for both drive cycles fuel saving due to the application of the electric water pump is relatively small and amounts between 0.75% and 1.1%. However, it is important to address that application of the electric coolant pump results in higher turbine outlet temperatures and thus in faster catalyst heat-up. Detailed analyses of the interaction between vehicle dynamics, transient engine performance and engine thermal management system provide insight into the underlying mechanisms. This is made possible by the application of physically based system level model.", "abstract": "A physically based approach to model vehicle dynamics, transient engine performance and engine thermal management system is presented. This approach enables modeling dynamic processes in the individual components and is the dynamic interaction of all relevant domains. The modeling framework is based on a common innovative solver, where all processes are solved using tailored numerical techniques suited to account for characteristic time scales of individual domains. This approach enables achieving very short computational times of the overall model. The paper focuses on the integration of cooling and lubrication models into the framework of a vehicle dynamics simulation including transient engine performance demonstrated on a modern passenger car featuring split cooling functionality. A validated model with a mechanically driven coolant pump provides the base for analyzing the impact of introducing an electrically driven coolant pump. Analyses are performed for two drive cycles featuring significantly different velocity profiles to reveal their influences on the operational principles of the powertrain components and their interaction. The results show for both drive cycles fuel saving due to the application of the electric water pump is relatively small and amounts between 0.75% and 1.1%. However, it is important to address that application of the electric coolant pump results in higher turbine outlet temperatures and thus in faster catalyst heat-up. Detailed analyses of the interaction between vehicle dynamics, transient engine performance and engine thermal management system provide insight into the underlying mechanisms. This is made possible by the application of physically based system level model.", "introduction": "Global concerns on sustainable energy use and environmental protection call for innovative powertrain technologies. Optimizing the vehicle thermal management is considered as one of the measures to reduce fuel consumption and to lower exhaust emissions. This paper therefore analyzes differences in fuel consumption and in catalyst heat up caused by two different approaches for propelling and controlling the coolant pump, a mechanical belt/gear propulsion and an electric motor propulsion are compared.Different topologies of electrified cooling system for internal combustion engines are investigated in [1\u20133], where results indicate that advanced thermal topologies can reduce fuel consumption for 1\u20133% through lower parasitic losses of the coolant pump and minimized coolant temperature fluctuations. In [4] the authors focus on mechatronic components used in cooling system (ie smart thermostat valve, electric coolant pump, variable speed radiator fan), on proper controlling and on synchronized data exchanges between selected components as all these aspects are important to realize fuel economy enhancements under different driving scenarios. Similarly, in [5] the proper controllability of components is exposed as the key element in achieving enhanced fuel economy. It is further reported in [5] that when the electric water pump runs with a higher coolant temperature set point of 110\u00b0C, a fuel reduction of 2\u20135% is achieved compared to the baseline set point of 90\u00b0C. This is also in line with results presented in [6], reporting that higher coolant temperatures reduce brake specific fuel consumption (BSFC) and that the warm-up period can be shortened by maximum 25% through a zero flow strategy of the coolant pump. In [7] the authors present a theoretical analysis of an optimized cooling system applied to an internal combustion engine (ICE) powered passenger car. It was shown that the largest fuel saving potentials, even up to 6\u201328%, can be achieved in urban driving condition. These very high fuel saving values mainly originate from the fact that study is based on a theoretical assessment of potential improvements, which among others include the assumption of instantaneous engine warm-up. One of the main motivations to integrate an electric coolant pump is a significantly reduced power consumption of the electrically driven over the mechanically driven pump. The study [8] reports that the electrically driven pump reduces the energy consumption by 99% compared to mechanically driven pump. This reduction mainly arises from operation of the coolant pump at much lower speeds being correlated with the actual cooling demand. However, in general it also needs to be considered that a minimum coolant flow should be ensured to avoid potential hot spots in engine as reported in [9].The assessment of vehicle thermal management requires simultaneous consideration of multiple interacting domains, since its response depends on the driving condition, vehicle parameters, engine operation and overlaying control strategies. System engineering simulations are widely applied to support analyses of complex interactions between components in multi domain models [8,10\u201313]. Complete vehicle models generally includes the domain of mechanical drive train, electrical circuit, internal combustion engine, exhaust aftertreatment, cooling circuit and corresponding control units. To efficiently support these analyses, system engineering models have to feature fast computational times in off-line (office) applications, as vast numbers of configurations need to be examined during early concept and design phases of the powertrain development. Besides the computational speed, accuracy and predictability also significantly influence the applicability of system engineering simulations. In the context of this paper predictability is the degree to which an adequate prediction can be made by the model without any prior model calibration. A high level of predictability is particularly required when evaluating powertrain performance in different operating regimes, where physically plausible response of individual components and adequate interaction between the components is mandatory.System level studies on vehicle thermal management systems that include their interaction with the engine performance and emissions parameters generally rely on the data driven models of particular domains, ie powertrain subsystems. Similarly, data driven engine models are also widely used in other system level studies, where besides steady state maps, eg [8,14,15], surrogate models are used also, eg [16]. Such models feature significant limitations when applied to transient conditions that are far from steady-state. This is even more pronounced for turbocharged engines with exhaust gas recirculation (EGR) as analyzed in [17]. To overcome these deficiencies hybrid engine models are often used combining surrogate approaches like Static Neural Networks [18] or Support Vector Machines [19] with physical based model parts. In addition, there have been several attempts to develop data driven transient emission models [14,15,20]. However, both steady-state and transient data driven models require upfront data for populating maps for training the surrogate models. These data might not yet be fully available at very early stages in the development process. It is therefore advantageous to use physical based models, eg [12,13,21\u201325], as they rely on first order principles and physically motivated correlations. In addition, they also inherently consider changes in engine controls, thermal behavior of the engine, variations in gas state and composition as well as turbocharger lagging via deterministic approaches. This enables more adequate modeling of the engine performance.In order to comply with the mentioned requirements on computational speed and predictability, this paper presents an advanced modeling approach that is capable of modeling dynamic processes in the individual components and dynamic interaction of all relevant domains using a common numerical solver framework. This brings advantages over the co-simulations of specialized tools for modeling particular domains due to the possibility to optimally couple different domains [12].This is reached by introducing an innovative tailored solver framework suited to account for the characteristic time scales of the individual domains and the domain coupling. All modeled domains do not depend only on current state, ie behave quasi-steady, but they also depend on the recent history, whereby each domain features one or more distinct characteristic time scales as presented in [13]. The paper at hand focuses on the integration of an Engine Thermal Management (ETM) model into a vehicle dynamics simulation [26,27] also taking into account transient engine performance [27\u201329].The modeling framework, as it includes a very detailed ETM model, is applied to analyze different thermal management approaches of a passenger car under a variety of realistic driving scenarios. The analysis is based on a validated model of a 1.2ton passenger car powered by a 4 cylinder 1.6l turbocharged gasoline direct injection engine (TGDI) that interacts with a split ETM circuit. A baseline cooling topology with a mechanically driven coolant pump is compared to a cooling topology incorporating electrically driven coolant pump. Both cooling topologies are assessed under different driving conditions to reveal drive cycle specific differences of the two cooling concepts.Modeling of system engineering configurations requires a careful selection of the physical depth of the models used to describe phenomena in particular domains to optimize the computational performance of the overall model. In addition, it is necessary to consider the characteristic time scales of the modeled phenomena to improve computational speed and numerical robustness by avoiding stiffness of the overall system matrix, as discussed in [13,26]. In such an approach, domains that are integrated with the same time steps represent sub-matrices in the overall system matrix. This time domain decoupling goes along with the application of dedicated routines that ensure inter-domain flux conservation. The characteristics of the individual domains are discussed in detail in the following sections.The vehicle model presented in Fig 1"}
{"conclusion": "The main results of this study point to the following conclusions:", "paper_id": "S1474034616300192", "author-highlights": "This paper proposes metrics to estimate the expected predictive performance of sensor configurations.The metrics evaluate the robustness of sensor configurations with respect to reducing uncertainty of model predictions.The evaluations are based on the premise that measurement data are best used for falsifying model instances.Potential of the predictive performance metrics is demonstrated using full-scale high-rise buildings in Singapore.", "main-title": "Evaluating predictive performance of sensor configurations in wind studies around buildings", "summary": "A great challenge associated with urban growth is to design for energy efficient and healthy built environments. Exploiting the potential for natural ventilation in buildings might improve pedestrian comfort and lower cooling loads, particularly in warm and tropical climates. As a result, predicting wind behavior around naturally ventilated buildings has become important and one of the most common prediction approaches is computational fluid dynamics (CFD) simulation. While accurate wind prediction is essential, simulation is complex and predictions are often inconsistent with field measurements. Discrepancies are due to the large uncertainties associated with modeling assumptions, as well as the high spatial and temporal climatic variability that influences sensor data. This paper proposes metrics to estimate the expected predictive performance of sensor configurations and assesses their usefulness in improving simulation predictions. The evaluations are based on the premise that measurement data are best used for falsifying model instances whose predictions are inconsistent with the data. The potential of the predictive performance metrics is demonstrated using full-scale high-rise buildings in Singapore. The metrics are applied to assess previously proposed sensor configurations. Results show that the performance metrics successfully evaluate the robustness of sensor configurations with respect to reducing uncertainty of wind predictions at other unmeasured locations.", "abstract": "A great challenge associated with urban growth is to design for energy efficient and healthy built environments. Exploiting the potential for natural ventilation in buildings might improve pedestrian comfort and lower cooling loads, particularly in warm and tropical climates. As a result, predicting wind behavior around naturally ventilated buildings has become important and one of the most common prediction approaches is computational fluid dynamics (CFD) simulation. While accurate wind prediction is essential, simulation is complex and predictions are often inconsistent with field measurements. Discrepancies are due to the large uncertainties associated with modeling assumptions, as well as the high spatial and temporal climatic variability that influences sensor data. This paper proposes metrics to estimate the expected predictive performance of sensor configurations and assesses their usefulness in improving simulation predictions. The evaluations are based on the premise that measurement data are best used for falsifying model instances whose predictions are inconsistent with the data. The potential of the predictive performance metrics is demonstrated using full-scale high-rise buildings in Singapore. The metrics are applied to assess previously proposed sensor configurations. Results show that the performance metrics successfully evaluate the robustness of sensor configurations with respect to reducing uncertainty of wind predictions at other unmeasured locations.", "introduction": "The continuous growth of the global population living in cities has increased interest in outdoor thermal comfort [1], air quality [2,3], safety [4], and particularly in warm climates, building energy consumption and natural ventilation [5]. The wind environment has a primary role in mitigating these issues and, consequently, improving knowledge of wind behavior around buildings has been the focus of much recent research work (a detailed review can be found in [6]). The most common approach for wind prediction is based on computational models, such as those used in computational fluid dynamics (CFD) simulations.Today, CFD simulations are used to overcome constraints of laboratory and field measurements [5], since they provide detailed information on wind flow and allow treatment of complex geometries with a high degree of repeatability. Although CFD simulations provide reasonable predictions, the accuracy is not superior to laboratory and field measurements [6]. Uncertainties are large in both modeling and measurements and should be taken into account [7].Tamura [8] and Schatzmann etal [9] suggested that measurements, both laboratory and field, should be used in a complementary manner to ensure that simulation results are sound, even when using modeling methods of high predictability, such as the large eddy simulation (LES) [8]. However, the use of simplified arrays of roughness elements in laboratory measurements results in idealized representations of the parameters affecting wind flow [10], and it is often unclear how sensitive wind predictions are to these parameter uncertainties [11]. Moreover, certain physical phenomena, such as buoyancy-driven natural ventilation, cannot be fully represented in reduced-scale laboratory experiments [5].In these situations, field measurements are essential for ensuring that modeling is sound, especially in studies involving high-rise buildings [10,11]. Nevertheless, field measurements have been rare, and sensor placement still remains a challenging task [12]. Most of earlier studies have used historically measured data and only a few configured full-scale measurement campaigns [13\u201316]. In most cases, sensors locations have been selected based on educated guesses, although some researchers have investigated optimal sensor configurations using systematic and data-driven strategies [17\u201320]. Such strategies require prior knowledge of data distributions and spatial correlations, obtained from a denser pre-deployment of sensors, a task that is however both costly and time-intensive. A recent study in Singapore has used the concept of maximum entropy for sensor selection, in a similar way to [21], yet modeling and measurement data were assumed to be free of errors and sensor placement was performed iteratively, disregarding the mutual information content between sensor locations.All of these studies employed data from sensor locations placed outside the urban canopy, or on building rooftops, although reports on methods to obtain representative data suggest that, unless interested in topographic-generated climate patterns, sensor locations subject to local and mesoscale effects should be avoided and target measurement areas of acceptable homogeneity at screen-level (\u223c1.5m) or high-level (about the roughness sub-layer) should be selected [22]. Wind flow varies considerably over space and time and measurements within the urban canopy depend on the location of sensors and sampling frequency [23,24]. In addition, it has been shown [7] that even under steady ambient conditions, large discrepancies occur between measured and predicted values that are caused by low frequency variations of the wind flow.A recent study done by the authors explored systematic sensor placement strategies that are applicable to time-dependent wind prediction within the urban canopy, involving buildings of varying size and use [25]. The study adapted and compared sequential strategies and criteria used in the field of infrastructure diagnosis, which can achieve high levels of accuracy with low computational cost compared to global search strategies [26] and genetic algorithms [27]. The typical information-based criteria found in literature for optimal sensor configuration were information entropy [26\u201328], cost and expected identifiability [29], while some studies incorporated uncertainty correlations and their values [29,30]. Based on the conclusions of the authors\u2019 initial study [25], a novel hierarchical sensor placement strategy has been developed that uses the concept of joint-entropy to account for the mutual information between sensor locations [31]. The strategy also explicitly incorporated the spatial distributions of modeling errors and their values, which has been shown to affect optimal sensor configuration [30\u201332].In conclusion, typical information-based criteria used for optimal sensor configuration focused primarily on increasing information value, measured either with entropy from information theory or using identifiability metrics to reduce parameter-value uncertainty. Nonetheless, earlier studies [33] have suggested that the performance of sensor configurations in reducing uncertainty of model predictions should be assessed by additional criteria, such as robustness-to-uncertainty and \u201cprediction-looseness\u201d\u2014equivalent to the range of predictions\u2014which are often conflicting.This paper proposes metrics to evaluate the predictive performance of sensor configurations and assess their usefulness in improving simulation predictions. The performance of the configurations is assessed for their capability to falsify multiple model instances whose predictions are inconsistent with the data (Section 2.1). Expected identifiability metrics found in literature [29] are adapted (Section 2.2) and then new metrics are developed (Section 2.3) to estimate the sensor configurations robustness-to-uncertainties associated with model predictions. In the end, a multi-criteria decision-making (MCDM) approach is proposed to evaluate the influence of the conflicting metrics on the choice of optimal sensor configuration (Section 2.4). In Section 3, the proposed metrics are applied to evaluate the performance of several hierarchically-constructed sensor configurations [31], in improving wind predictions around a full-scale building system in Singapore. A list of the main conclusions and a critical assessment on the results are provided in the final two sections (Sections 4 and 5).Metrics are developed to estimate the expected predictive performance of sensor configurations and assess their usefulness in improving predictions. The study builds upon previous work on sensor placement performed by the authors [25,31], where the premise is that sensor data are best used for falsifying multiple model instances"}
{"conclusion": "Left ventricular hypertrophy has been recognized as an active adaptation of left ventricular wall tissues to chronic hemodynamic loads. Chronic high sodium intake may induce hypertension and therefore is closely related to left ventricular hypertrophy. In the present study, a mathematical model has been developed to study the effects of high sodium intake on the cardiovascular hemodynamics and the body-fluid homeostasis. The new hybrid model couples and modifies a cardiovascular model and a long-term renal system model. The model has been validated using clinical data. The results indicate that the new model is effective in the simulation of the long-term adaptations of hemodynamic and homeostasis parameters.The simulation results suggest that with an increase of the sodium intake, body fluid volume, blood pressure, and different hormone concentrations adapt to new levels of balances. As the sodium intake increases, the left ventricular pressure and volume increase, particularly during systole. The left ventricle work load increases considerably. The left ventricle relative wall thickness increases as the sodium intake increases. There is evidence that the impaired down-regulation of ANG-II, which is not simulated in the current model, is also correlated to the left ventricle wall thickness in left ventricular hypertrophy patients. As the exact mechanism of ANG-II-regulated left ventricular hypertrophy becomes clear in the future, mathematical modeling of the ANG-II effects on left ventricle wall thickness can be developed.In summary, the current model can reasonably simulate the long-term response of the cardiovascular and the renal systems parameters under different chronic sodium intakes examined. With an explicit modeling of the long-term hemodynamics changes, the model can be used for the estimation of the increase of left ventricle wall thickness due to pressure and volume loading induced by the increase of sodium intake.None declared.", "paper_id": "S0010482514003618", "author-highlights": "A system-level computer simulation of chronic high sodium intake effects is conducted.The current model couples a cardiovascular hemodynamics model and a renal system model.Left ventricular (LV) wall stress is modeled to compute the changes of the LV wall thickness.Modeling results suggest high sodium intake alters the body-fluid homeostasis and increases the LV work load and relative wall thickness.", "main-title": "Modeling of high sodium intake effects on left ventricular hypertrophy", "summary": "Many clinical studies suggest that chronic high sodium intake contributes to the development of essential hypertension and left ventricular (LV) hypertrophy. In the present study, a system-level computer model has been developed to simulate the long-term effects of increased sodium intake on the LV mechanical functions and the body-fluid homeostasis. The new model couples a cardiovascular hemodynamics function model with an explicit account of the LV wall thickness variation and a long-term renal system model. The present model is validated with published results of clinical studies. The results suggest that, with increased sodium intake, the renal system function, the plasma hormone concentrations, and the blood pressure adapt to new levels of equilibrium. The LV work output and the relative wall thickness increase due to the increase of sodium intake. The results of the present model match well with the patient data.", "abstract": "Many clinical studies suggest that chronic high sodium intake contributes to the development of essential hypertension and left ventricular (LV) hypertrophy. In the present study, a system-level computer model has been developed to simulate the long-term effects of increased sodium intake on the LV mechanical functions and the body-fluid homeostasis. The new model couples a cardiovascular hemodynamics function model with an explicit account of the LV wall thickness variation and a long-term renal system model. The present model is validated with published results of clinical studies. The results suggest that, with increased sodium intake, the renal system function, the plasma hormone concentrations, and the blood pressure adapt to new levels of equilibrium. The LV work output and the relative wall thickness increase due to the increase of sodium intake. The results of the present model match well with the patient data.", "introduction": "Left ventricular hypertrophy (LVH), manifested by an enlargement of myocardium tissue, is a predisposing factor for many cardiovascular diseases [1]. During the past decades, numerous studies have demonstrated a strong correlation between chronic hypertension and LVH [2\u20135]. A physiologic adaptation of the left ventricular structure to the chronic pressure loading is generally considered the main reason for hypertrophy [4,6]. Following the Laplace\u05f3s law, this adaptation allows the wall stress and the pumping function to remain relatively unchanged by increase in the left ventricle (LV) tissue volume and LV wall thickness [7].As a major contributor to the prevalence of essential hypertension [8,9], high sodium intake has been demonstrated to cause LVH by inducing chronically elevated blood pressure [10\u201312]. Meanwhile, some studies also suggest that high sodium intake can also contribute to LVH through a different route, independent of or in addition to the hypertensive effect [13\u201318]. In patients with essential hypertension and high sodium intake, an insufficiently down-regulated Angiotensin II concentration was found to correlate with cardiac hypertrophy [19]. Animal experiments also suggest that high sodium intake facilitates the activation of the local (cardiac) renin-angiotensin system (RAS) [20], increases the synthesis of cardiac aldosterone [21], and induces higher concentration of Angiotensin II in myocardium [22]. Based on these clinical findings, sodium restriction [23] and angiotensin-converting enzyme (ACE) inhibitor [24] have been used with positive effects on the regression of LVH and therefore successfully prescribed as treatments for LVH [6].In the present study, a computer model has been developed to link high sodium intake to LVH based upon the existing clinical evidences. A long-term renal system model developed by Karaaslan etal [25] is adapted and used to simulate the chronic effects of high sodium intake on body-fluid homeostasis. To account for the LV pressure loading, we used an open-source hemodynamics model of human cardiovascular system, ie CVSim [26]. A model for the response of the LV wall thickness to the pressure overload is developed. By combining and modifying the renal system model and the CVSim model, the new model is capable of simulating both the long-term effects of body-fluid homeostasis and the detailed structural properties of the human cardiac chambers. Since clinical studies are often restricted with test subjects and other medical issues, computer modeling will be particularly helpful for an enhanced understanding of the system-level response of the LV wall thickness to high sodium intake. The details of the model will be described in the following section.CVSim is based on a closed-loop circulation model for hemodynamic responses of cardiovascular system, the total blood volume is kept constant and the body-fluid homeostasis is not simulated. For the purpose of the present study, we couple a long-term renal system model to the existing CVSim model. The hybrid model then has the capability of simulating both the short-term response of hemodynamics and the long-term fluid-electrolyte homeostasis.An open-source model, named CVSim, has been adapted and used for the hemodynamic simulation in the present study. Since 1984, CVSim has been developed and successfully utilized in teaching [26] and research [27\u201328]. This model is a closed-loop, lumped-parameter model based on electrical circuitry analogy. As shown in"}
{"conclusion": "We demonstrated a high speed pixel circuit for high resolution and frame rate AMOLED displays where the voltage programming method is consolidated to compensate the threshold voltage variation. The proposed 7T1C pixel circuit outperforms the previous 5T2C circuit in the points of view of uniformity, pixel area, and power consumption. Especially, the SMART-SPICE simulation ensures that the uniformity and compensation voltage error of a proposed 7T1C circuit is reduced to 2.5% and 0.18V, on the average, compared to 7.1% and 0.75V of the previous 5T2C circuit at the full-HD 240Hz display. In addition, the power consumption of the initialization and the dynamic power consumption of data drivers are reduced by 80% and 98.7% in the 7T1C scheme from that of the 5T2C one. As a consequence, the proposed 7T1C circuit can pave the way for the high resolution, high frame rate and low power AMOLED displays.", "paper_id": "S0141938214000298", "author-highlights": "A novel 7T1C pixel circuit is proposed for high resolution, high frame rate, and low power AMOLED displays.The whole line time is in use only for charging the data voltage.The current non-uniformity is reduced to 2.5% at a 240Hz full-HD AMOLED display.The average compensated voltage error is reduced to 0.18V, compared to 0.75V of 5T2C.The initialization power consumptions are 98mW and the dynamic power saving is 98.7% over 5T2C.", "main-title": "Novel voltage programming n-channel TFT pixel circuit for low power and high performance AMOLED displays", "summary": "This paper proposes a novel pixel circuit for high resolution, high frame rate, and low power AMOLED displays that is implemented with one driving n-channel TFT, six switching n-channel poly-Si TFTs, and a storage capacitor. The proposed pixel circuit adopts the voltage programming scheme for threshold voltage compensation. Because the whole line time is in use only for charging the data voltage, this pixel circuit is applicable to high resolution and frame rate displays. In addition, it compensates voltage variation of OLEDs and voltage drop of supply lines at lower power consumption. On the average, the non-uniformity of a proposed circuit is reduced to 2.5%, compared to 7.1% of the previous one at a 240Hz full-HD display. On the other hand, the compensation voltage error, which is caused by feed-through and charge injection noises from falling control signals of switching TFTs, is much less in the proposed scheme than in the previous 5T2C structure. The average error of the proposed circuit is reduced to 0.18V, compared to 0.75V of the previous one. The initialization power consumption of the 7T1C circuit is reduced to 98mW, compared to 530mW of the 5T2C circuit and the average dynamic power saving ratio of data drivers is estimated in the 7T1C pixel as 98.7% over the 5T2C one for 24 test images.", "abstract": "This paper proposes a novel pixel circuit for high resolution, high frame rate, and low power AMOLED displays that is implemented with one driving n-channel TFT, six switching n-channel poly-Si TFTs, and a storage capacitor. The proposed pixel circuit adopts the voltage programming scheme for threshold voltage compensation. Because the whole line time is in use only for charging the data voltage, this pixel circuit is applicable to high resolution and frame rate displays. In addition, it compensates voltage variation of OLEDs and voltage drop of supply lines at lower power consumption. On the average, the non-uniformity of a proposed circuit is reduced to 2.5%, compared to 7.1% of the previous one at a 240Hz full-HD display. On the other hand, the compensation voltage error, which is caused by feed-through and charge injection noises from falling control signals of switching TFTs, is much less in the proposed scheme than in the previous 5T2C structure. The average error of the proposed circuit is reduced to 0.18V, compared to 0.75V of the previous one. The initialization power consumption of the 7T1C circuit is reduced to 98mW, compared to 530mW of the 5T2C circuit and the average dynamic power saving ratio of data drivers is estimated in the 7T1C pixel as 98.7% over the 5T2C one for 24 test images.", "introduction": "Active matrix organic light emitting diode (AMOLED) displays have been designed in portable devices such as mobile phones and tablet PCs on the back of fast response time, wide color gamut, wide viewing angle, high contrast ratio, slim and light module, and low material cost [1]. Even though AMOLED TVs have been mass-produced in 2013, there exist several issues in AMOLED displays, such as non-uniformity, the voltage drop on the supply line, and power consumption.A basic AMOLED pixel circuit consists of one driving transistor and one switching transistor at the backplane of poly silicon (poly-Si) thin film transistors (TFTs) which have much higher mobility than amorphous silicon (a-Si). However, because a poly-Si TFT suffers from the threshold voltage variation over the area, the luminance non-uniformity problem is observed on the screen without any additional compensation circuitries [2]. This non-uniformity issue has improved by voltage programming, current programming, digital driving, and external programming schemes [3\u20136].Whereas p-channel poly-Si TFTs have been widely employed due to the hot carrier issue of n-channel TFTs and the OLED implementation issue [7], some previous papers have proposed an AMOLED pixel circuit with n-channel poly-Si TFTs. Unlike the liquid crystal pixels consume no current after charging pixel electrodes, AMOLED pixels continue to flow the current from the supply line. As a consequence, AMOLED pixels experience the different supply voltages according to the position, which in turn, lead to the non-uniform luminance issue in a p-channel TFT circuit [8]. An n-channel TFT circuit can cope with supply voltage drop without additional circuitries because the supply is connected to a drain terminal in a saturation region. Also, the n-channel TFT pixel circuits can reduce the supply voltage of data drivers, which leads to lower power consumption [9], since the current is programmed by the voltage difference between data and low supply whereas the p-channel TFT circuits make use of the difference between data and high supply voltages. However, the n-channel circuits have to connect the anode of an OLED to the source of a driving TFT. In that the current is affected by the voltage across an OLED, the n-channel pixel structure and programming scheme can be more complicated, compared to p-channel TFT circuit. Tai introduced a voltage programming pixel circuit which utilized one n-channel driving TFT, three n-channel switching TFTs, one p-channel switching TFT, and one storage capacitor [10]. Although the gate voltage of a driving TFT can be directly programmed by data voltage with only two control signals, the additional initialization period within each line time hinders the pixel circuit from increasing its operating frequency. Furthermore, the voltage variation across an OLED cannot be compensated and the fabrication cost increases due to CMOS process. Lee proposed an n-channel a-Si:H TFT pixel circuit which consists of six TFTs and one storage capacitor with two control signals [11]. It has the same frequency issue due to the initialization period employed within a line time. It is hard to execute the initialization in the other period of time since the data line voltage level is changed by the connection to VDD and VSS through TFTs. On top of that, because the negative data voltage is required to program the current, the negative voltage should be supplied to power-consuming data drivers. Jung introduced the threshold voltage compensation scheme for both driving TFT and OLED with five TFTs and one capacitor [12]. This structure must discharge capacitor and OLED with very low current through a driving TFT in one line time. Therefore, the sufficient compensation cannot be achieved in high resolution and frame rate displays. Moreover, there is another problem that the initial gate voltage may be too low to turn on a driving TFT in the compensation period according to the image sequences. Wu proposed an AMOLED pixel circuit of five n-channel TFTs and two capacitors (5T2C), where C"}
{"conclusion": "In this paper we proposed a new smart sampling system for cognitive radio, called the Dynamic Single Branch non-uniform sampler. To ensure optimal reconstruction with a small number of samples, the DSB adapts its parameters according to the input signal\u2019s sparsity. Its operates in two phases, the adaptation phase and the reconstruction phase. In the adaptation phase, the proposed scheme senses the spectrum and adapts its sampling rate by computing the proposed non-uniform spectrum sensing technique. We have shown that the proposed sensing model works efficiently and shows high detection and low false alarm probabilities. The performance of the spectrum sensing model improves by increasing the number of available non-uniform samples to the sensing method. As the sampling instances of our Non-uniform sampler depends to the position of this one on the Nyquist grid, then the average sampling rate depends on the number of bands contained in the signal. Furthermore, we have shown the effect of false detection on it. The DSB sampler performance has been compared to those of a traditional, non flexible Multi-Coset architecture which contains severals parallel uniform samplers. We have shown that our system is significantly more efficient than the conventional MC sampler when the spectrum of signal changes. The system can be designed with existing ADC and falls in the category of software defined radio system.", "paper_id": "S0045790615001512", "author-highlights": "We have proposed a new smart sampler scheme to reduce the power consumption of the Analog to Digital Converter by reducing its average sampling frequency.Its more efficient, in terms of sampling rate, than classical sampler (uniform or multi-coset) when the false detection of bands localisation is low.", "main-title": "Adaptive non-uniform sampling of sparse signals for Green Cognitive Radio", "summary": "Based on previous results on periodic non-uniform sampling (Multi-Coset) and using the well known Non-Uniform Fourier Transform through Bartlett\u2019s method for Power Spectral Density estimation, we propose a new smart sampling scheme named the Dynamic Single Branch Non-uniform Sampler. The idea of our scheme is to reduce the average sampling frequency, the number of samples collected, and consequently the power consumption of the Analog to Digital Converter. In addition to that our proposed method detects the location of the bands in order to adapt the sampling rate. In this paper, through we show simulation results that compared to classical uniform sampler or existing multi-coset based samplers, our proposed sampler, in certain conditions, provides superior performance, in terms of sampling rate or energy consumption. It is not constrained by the inflexibility of hardware circuitry and is easily reconfigurable. We also show the effect of the false detection of active bands on the average sampling rate of our new adaptive non-uniform sub-Nyquist sampler scheme.", "abstract": "Based on previous results on periodic non-uniform sampling (Multi-Coset) and using the well known Non-Uniform Fourier Transform through Bartlett\u2019s method for Power Spectral Density estimation, we propose a new smart sampling scheme named the Dynamic Single Branch Non-uniform Sampler. The idea of our scheme is to reduce the average sampling frequency, the number of samples collected, and consequently the power consumption of the Analog to Digital Converter. In addition to that our proposed method detects the location of the bands in order to adapt the sampling rate. In this paper, through we show simulation results that compared to classical uniform sampler or existing multi-coset based samplers, our proposed sampler, in certain conditions, provides superior performance, in terms of sampling rate or energy consumption. It is not constrained by the inflexibility of hardware circuitry and is easily reconfigurable. We also show the effect of the false detection of active bands on the average sampling rate of our new adaptive non-uniform sub-Nyquist sampler scheme.", "introduction": "Radio Frequency (RF) allows modulation of narrow band signals with a high carrier frequency. The radio signals of human origin are often sparse. In other words, they are composed of a relatively small number of narrow band transmissions spread across a wide spectral region. A practical description of these signals is the multi-band model where the spectrum of the signal is only composed of several continuous intervals in a wide spectrum. In addition, new wireless applications place high demands on the quality of radio resources such as bandwidth and spectrum. Moreover, the current trends in wireless technology have increased the complexity of the receiver, more specifically its Analog to Digital Converter (ADC). According to Shannon\u2013Nyquist theorem, a signal whose spectral support is limited to"}
{"conclusion": "In this paper we present a fully-automated method for the breast-region segmentation in the axial MR images. The main advantage of our method compared to other methods proposed in the literature for the axial MR images is in its applicability for the challenging cases where a part of the fibroglandular tissue is connected to the chest wall and/or skin with no visible contrast, ie no fat presence, between them. The important advantage is also in not requiring any training set for learning the anatomical or statistical knowledge which makes the segmentation inherently dependable on the intrinsic properties of the training data. The method is validated on a representative database covering the full range of the BI-RADS breast-density categories indicating that the method allows for an accurate and robust segmentation. It can be concluded that the proposed method shows a potential to be incorporated into the computer-aided analysis system for the breast MRI to support physicians in their decision making.None declared.", "paper_id": "S0010482515001195", "author-highlights": "The method for the breast-region segmentation in the axial MR images is proposed.It uses the shortest-path search incorporating information from the adjacent slice.The cost function incorporates the edges obtained using a tunable Gabor filter.The method is applicable for cases with no visible contrast at breast-region border.The method requires no training procedure.", "main-title": "Automated breast-region segmentation in the axial breast MR images", "summary": "Purpose", "abstract": "Purpose", "introduction": "Magnetic Resonance Imaging (MRI) is an invaluable tool in the clinical work-up of patients suspected of having breast cancer [1,2]. Moreover, the breast MRI is gaining popularity as a screening modality for patients with dense breasts or high-risk patients with BRCA 1 & 2 gene mutations (screened at a younger age when their breasts are dense) [3,4]. As the diagnostic efficiency is highly dependent on the level of experience of the radiologist [5,6], recent researches have focused on developing computer-aided analysis methods aimed at helping the radiologists in their diagnostic tasks, particularly those radiologists experienced in mammography reading but less experienced in interpreting the breast MRI [7].The automated breast-region segmentation is required for performing a fully-automated computer-aided analysis of the breast MR images overcoming drawbacks of the manual and user-assisted segmentation which are impractical for processing large amounts of the MRI data being time consuming and biased by both the intra-observer and inter-observer variability. The breast-region segmentation refers to separation of the breast as an organ from other body parts in the MR images. The importance of the automated breast-region segmentation has recently been recognized in a number of applications for the breast MRI, such as the breast-density measurement [8] and tumour detection for the subsequent tumour classification [9]. It can also facilitate the pharmacokinetic-model calibration with respect to the reference tissues for improving the diagnostic performance of the dynamic contrast-enhancement breast tumours [10] where, for the purpose of calibration, the pectoral muscle (the most anterior chest-wall muscle) can be used given its properties [11]. This would require, among others, a precise breast-chest wall border determination which is a part of the breast-region segmentation.To automatically segment the breast-region in the MR images, the segmentation algorithm needs to determine the breast-chest wall and breast-skin borders as well as the lateral-posterior breast ends. In the T1-weighted MR images \u2013 which are almost always included in the typical clinical breast MRI protocols for tumour analysis and used for the breast-density measurement \u2013 the fibroglandular tissue, skin and chest wall have a similar signal intensity. Therefore, for the challenging cases where a part of the fibroglandular tissue is connected to the chest wall and/or skin resulting in no visible contrast between them, automated breast-region segmentation is a highly demanding task. Moreover, the MRI signal intensity is usually affected by inhomogeneity, partial volume effect, aliasing, ghosting artefacts, etc, furtherly aggravating the breast-region segmentation.There have been a few methods proposed in the literature to automatically segment the breast region from the MR images. The simplest ones are based on the intensity threshold usually followed by morphological operations [12,13]. The more advanced ones, such as fuzzy C-mean (FCM) clustering [14], sign of gradients [9,15], region growing [16,17], and Markov random field [18], use various connectivity rules between pixels. These methods are highly dependent on the visible contrast between the breast region and chest wall/skin, ie on the presence of the fat along the breast-region border, and therefore tend to fail in the challenging cases.The performance of the model-based [19] and atlas-based [20,21] methods for the breast-region segmentation depends on the size and variety of the training database which is critical in achieving a reasonable accuracy. Moreover, the atlas-based segmentation techniques rely on an accurate intensity-based registration of the image to be segmented with the population atlas where the intensity-based registration assumes a similar grey-level distribution in the image compared to the atlas which it is impossible to account for the wide range of the clinical acquisition protocols (eg, T1-weighted, T1-fat suppressed, T2-weighted), while it is impractical to create high-quality, expert-annotated atlases for every different acquisition protocol [19]. The absence of one or both breasts, as a result of the mastectomy surgery, might also challenge the model-based methods for the breast-region segmentation. Moreover, the previously proposed atlas-based methods have not been validated for the challenging cases while the proposed model-based method has failed in the cases when the contrast between the breast and chest wall is lower than the contrast between the chest wall and chest.The edge-based methods [17,22,23] for the breast-chest wall border determination incorporate classical edge-detectors, such as Canny and Hessian, that are highly sensitive to the noise, low contrast and weak-textural edges affecting the accuracy of the breast-region segmentation. Moreover, the edge-based methods developed for the axial MR images [17,23] rely on the presence of the fat along the anterior side of the chest wall (according to their authors) and, therefore, they are not applicable for segmentation of the challenging cases.In this paper we present a novel fully-automated method for segmenting the breast region in the axial MR images using a tunable Gabor filter which sets its parameters according to the local MR image characteristics to detect non-visible transitions between different tissues having a similar MRI signal intensity. The breast-chest wall border is determined using a multi-slice approach where the border information from the adjacent slice is used to specify the border-search area in the current slice due to the high similarity in the chest-wall morphology (shape and inner structure) between two adjacent breast MRI slices. The method applies the shortest-path search technique within the border-search area by incorporating a novel cost function using the edges obtained with a tunable Gabor filter and subsequent extension of the resulting path along the fat up to the lateral-posterior breast ends. The lateral-posterior breast ends are determined using the body-contour landmarks, while the breast-skin border is determined by incorporating the edge information of the tunable Gabor filter in a slice-wise manner. Validated on 52 MRI scans covering the full range of American College of Radiology Breast Imaging-Reporting and Data System (BI-RADS) breast-density categories, our method shows to be independent from the visible contrast between the breast-region and surrounding chest wall and skin, ie, independent from the presence of the fat along the breast-region border.Our paper is organized as follows. After presenting the aim of our research and overview of the works related to the breast-region segmentation in Section 1, Section 2 describes the clinical breast MRI scans used in our research. Section 3 provides a detailed explanation of the proposed method for the breast-region segmentation. Section 4 introduces the validation metrics used to measure the segmentation accuracy. Section 5 reports our experimental results. Section 6 discusses the main findings of our research, outlines its limitations and determines the areas of our future work. Finally, Section 7 draws conclusions of our research.The used database of 52 pre-contrast MRI breast scans (ie volumes) was obtained from the Institute of Oncology, Ljubljana, Slovenia. These were all clinical cases of different patients where the screening MRI revealed a lesion suspected of being malignant. The age of the patients included in our database ranged from 28 to 67 years with an average of 46 years. The patients were scanned in a prone position.The axial T1-weighted images were acquired using a 3D fast low-angle shot-pulse sequence (FLASH) through both breasts (TR/TE 7.8/4.72, flip angle 25\u00b0) at 1.5T (Magnetom Avanto, Siemens, Erlangen, Germany) with a dedicated bilateral breast-surface coil in the prone position. The single-slice dimensions were 448\u00d7448, the field of view (FOV) was 340\u00d7340mm2 and the in-plane resolution was 0.76\u00d70.76mm2 with the slice thickness of 1mm. Each MRI scan counted 144 slices.The database covers the full range of the BI-RADS breast-density categories. With a consensus of two radiologists experienced in interpreting the breast MRI, the breasts in our database were classified into four categories: I\u2014extremely fatty, II\u2014minimally dense, III\u2014heterogeneously dense, and IV\u2014extremely dense breast (I: <25%; II: 25\u201350%; III: 51\u201375%; IV: >75%). Our database contained 14, 15, 12, 11 cases of the BI-RADS density categories I, II, III, and IV, respectively. All the scans from category IV and some scans from category III from our database contain the slices where a part of the fibroglandular tissue is connected to the chest wall and/or to the skin with no visible contrast between them, however, the scans from category IV contain a considerably larger number of these slices compared to the scans from category III. The scans from category I and II do not contain such slices.The manual segmentation of the breast region \u2013 considered as a gold standard (ie, ground truth) \u2013 was performed by two experienced radiologists (K.H., 18 years of experience, and M.M., 15 years of experience) by using the MIPAV software version 7.0.1 [24], specifically utilizing the Draw polygon VOI tool making a sequence of the left-clicks with the mouse along the breast-chest wall and breast-skin border. MIPAV automatically connects the points along each border, thus forming two contours giving rise to a binary mask corresponding to both borders (the border binary mask). In order to accommodate the varying sizes of the breast in different slices and MRI scans, the radiologists use a varying number of points to obtain a relatively smooth contour along both borders. The generated border binary mask is then stored and the procedure is repeated for each slice of the MRI scan. One half of the MRI scans was randomly assigned to one and the other half to another radiologist for manual delineations that were then all reviewed by both radiologists for error checking."}
{"conclusion": "We have constructed explicit formulas for \u03bc-bases of planar rational cubic curves and degree n rational curves in n-dimensions", "paper_id": "S0167839615001351", "author-highlights": "We provide explicit formulas for the\u03bc-bases of conic sections and planar rational cubic curves.We derive explicit formulas for the implicit equations and double points of planar rational cubic curves from the\u03bc-bases of planar rational cubic curves.We also give explicit formulas for the\u03bc-bases of rational curves of degreeninn-dimensions.", "main-title": "Explicit \u03bc-bases for conic sections and planar rational cubic curves", "summary": "We derive explicit formulas for the \u03bc-bases of conic sections and planar rational cubic curves. Using the \u03bc-bases for planar rational cubic curves, we find explicit formulas for their implicit equations and double points. We also extend the explicit formula for the \u03bc-bases of conic sections to \u03bc-bases for rational curves of degree n in n-dimensions.", "abstract": "We derive explicit formulas for the \u03bc-bases of conic sections and planar rational cubic curves. Using the \u03bc-bases for planar rational cubic curves, we find explicit formulas for their implicit equations and double points. We also extend the explicit formula for the \u03bc-bases of conic sections to \u03bc-bases for rational curves of degree n in n-dimensions.", "introduction": "\u03bc-bases for rational curves are important since we can use \u03bc-bases to implicitize rational planar curves (Chen and Wang, 2003). Moreover, \u03bc-bases for rational curves in arbitrary dimensions can be applied to locate and analyze their singularities (Song etal, 2007; Jia and Goldman, 2009, 2012; Shi and Chen, 2010). There are fast algorithms to compute \u03bc-bases for rational curves based mainly on Gaussian Elimination (Chen and Wang, 2003; Song and Goldman, 2009). However, we do not know what the \u03bc-bases look like before we run the algorithm; this drawback prevents us from finding closed formulas for the singularities or the implicit equation for rational curves based on the method of \u03bc-bases.Here we give explicit formulas for the \u03bc-bases of conic sections and planar rational cubic curves. As fundamental objects in Computer Aided Geometric Design and Computer Graphics, conic sections and planar rational cubic curves have a wide range of applications, for example, in animation control and font design. Implicit equations for planar rational cubic curves will also help us when we deal with intersection algorithms (Sederberg and Parry, 1986; Thomassen, 2005).Special attention has been devoted to computing implicit equations for planar rational cubic curves. Sederberg etal (1985) treat planar rational cubic curves as monoids, that is, degree n curves with a singular point of order"}
{"conclusion": "There are a large number of SMEs all over the world undergoing financing constraints, which have been inducing a huge loss of social welfare for a long time. This problem has been attracting much attention from researchers and practitioners.In this paper, we solve an SME\u2019s problem of pricing and timing of the option to invest in a project, which generates the cash flow following a double exponential jump-diffusion process. The SME has a funding gap to start the project and the gap is financed by entering into a partial guarantee agreement. We provide an explicit solution of the option value and the investment threshold. We show that the option value and investment threshold increase with project risk. If the funding gap rises, the option value decreases but the investment threshold first declines and then increases. The larger the guarantee level, the lower the option value and the later the investment. The optimal coupon rate and optimal leverage decrease with project risk. At a given fixed guarantee fee rate, the guarantee level and project risk have an ambiguous effect on the fraction of equity allocated to the insurer if coupon rate varies accordingly to keep capital structure optimal. While debt overhang is independent of guarantee levels, the inefficiency arising from asset substitution decreases as guarantee levels rise.In essence, it is the most important role played by a partial guarantee that the guarantee succeeds in exchanging a partial future cash flow of an SME for cash available at investment time to finance the SME\u2019s funding gap. In this way, financing constraints are in fact completely eliminated. From this perspective, the partial guarantee we discuss here is similar to a mortgage loan agreement, which has greatly improved our welfare level.Clearly, Eq (1) has the following unique solution:", "paper_id": "S0377221715008747", "author-highlights": "Utilizing a real options approach, we develop an investment and financing model with a partial guarantee.We explicitly derive the pricing and timing of the option to invest for the cash flow with both diffusion and jump risk.If the funding gap rises, the option value decreases but the investment threshold first declines and then increases.The larger the guarantee level, the lower the option value and the later the investment.Raising guarantee levels reduce borrowers\u2019 risk-shifting incentives but do not change their incentives to replenish equity.", "main-title": "Investment and financing for SMEs with a partial guarantee and jump risk", "summary": "We consider a small- and medium-sized enterprise (SME) with a funding gap intending to invest in a project, of which the cash flow follows a double exponential jump-diffusion process. In contrast to traditional corporate finance theory, we assume the SME is unable to get a loan directly from a bank and hence it enters into a partial guarantee agreement with an insurer and a lender. Utilizing a real options approach, we develop an investment and financing model with a partial guarantee. We explicitly derive the pricing and timing of the option to invest. We find that if the funding gap rises, the option value decreases but its investment threshold first declines and then increases. The larger the guarantee level, the lower the option value and the later the investment. The optimal coupon rate decreases with project risk and a growth of the guarantee level can effectively reduce agency conflicts.", "abstract": "We consider a small- and medium-sized enterprise (SME) with a funding gap intending to invest in a project, of which the cash flow follows a double exponential jump-diffusion process. In contrast to traditional corporate finance theory, we assume the SME is unable to get a loan directly from a bank and hence it enters into a partial guarantee agreement with an insurer and a lender. Utilizing a real options approach, we develop an investment and financing model with a partial guarantee. We explicitly derive the pricing and timing of the option to invest. We find that if the funding gap rises, the option value decreases but its investment threshold first declines and then increases. The larger the guarantee level, the lower the option value and the later the investment. The optimal coupon rate decreases with project risk and a growth of the guarantee level can effectively reduce agency conflicts.", "introduction": "Motivation. Small- and medium-sized enterprises (SMEs, henceforth) are the engine of the world economy. They provide a large number of job opportunities and create entrepreneurial spirit and technique innovation. Thus, they are crucial for fostering competitiveness and employment. Unfortunately, SMEs are severely limited by borrowing constraints when they have opportunities to invest in a project for business expansion. Particularly, they might not be able to borrow from banks at all, or they are offered with unfavourable lending conditions. As a result, they have to abandon potentially valuable investment opportunities. This situation becomes worse after the recent financial crisis. As reported by World Business Environment Survey, on average 43 (resp. 11) percent of businesses with 20 to 99 employees rate access to finance or cost of finance as a major constraint in developing (resp. developed) countries."}
{"conclusion": "This paper focuses on the 124 fold types of LIFCA, using the up to date functional domain composition which has not be used in fold recognition to predict the fold types of a protein or a domain. Results derived were highly accurate for the different test sets. For the samples which involve SCOP family divided, the success rate was not as good as the others. However, overall, the functional domain composition can be a promising method in fold recognition. And the result also indicated that though based on simple classification rules, LIFCA database can grasp the functional features of different proteins, reflecting the corresponding relation between protein structure and function.Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.compbiolchem.2013.12.001.", "paper_id": "S1476927113001175", "author-highlights": "A method based on functional domain composition to predict protein fold types is proposed.The result indicates this method is high-performance for protein fold recognition.LIFCA can reflect the corresponding relation between protein structure and function.", "main-title": "Protein fold recognition based on functional domain composition", "summary": "Recognition of protein fold types is an important step in protein structure and function predictions and is also an important method in protein sequence-structure research. Protein fold type reflects the topological pattern of the structure's core. Now there are three methods of protein structure prediction, comparative modeling, fold recognition and de novo prediction. Since comparative modeling is limited by sequence similarity and there is too much workload in de novo prediction, fold recognition has the greatest potential. In order to improve recognition accuracy, a recognition method based on functional domain composition is proposed in this paper. This article focuses on the 124 fold types which have more than 2 samples in LIFCA database. We apply the functional domain composition to predict the fold types of a protein or a domain. In order to evaluate our method and its sensibility to the samples involving SCOP family divided, we tested our results from different aspects. The average sensitivity, specificity and Matthew's correlation coefficient (MCC) of the 124 fold types were found to be 94.58%, 99.96% and 0.91, respectively. Our results indicate that the functional domain composition method is a very promising method for protein fold recognition. And though based on simple classification rules, LIFCA database can grasp the functional features of different proteins, reflecting the corresponding relation between protein structure and function.", "abstract": "Recognition of protein fold types is an important step in protein structure and function predictions and is also an important method in protein sequence-structure research. Protein fold type reflects the topological pattern of the structure's core. Now there are three methods of protein structure prediction, comparative modeling, fold recognition and de novo prediction. Since comparative modeling is limited by sequence similarity and there is too much workload in de novo prediction, fold recognition has the greatest potential. In order to improve recognition accuracy, a recognition method based on functional domain composition is proposed in this paper. This article focuses on the 124 fold types which have more than 2 samples in LIFCA database. We apply the functional domain composition to predict the fold types of a protein or a domain. In order to evaluate our method and its sensibility to the samples involving SCOP family divided, we tested our results from different aspects. The average sensitivity, specificity and Matthew's correlation coefficient (MCC) of the 124 fold types were found to be 94.58%, 99.96% and 0.91, respectively. Our results indicate that the functional domain composition method is a very promising method for protein fold recognition. And though based on simple classification rules, LIFCA database can grasp the functional features of different proteins, reflecting the corresponding relation between protein structure and function.", "introduction": "Research of protein 3D structures plays a key role in molecular biology, cell biology, biomedicine, and drug design (Burley, 2000). With the technological improvements in protein crystal structure determination, especially in diversified structure determination methods, experimental determination could be carried out at a much faster speed. However, experimental determination still cannot keep pace with increasing protein sequences. Therefore, it is important to develop new methods to predict the 3D structure from amino acid sequences in the post-genome era. Currently, there are three methods of protein structure prediction: comparative modeling, fold recognition, and de novo prediction (Baker and Sali, 2001). Given that comparative modeling is limited by sequence similarity and there is too much workload in de novo prediction, fold recognition has the greatest potential in predicting protein structures.Classification of protein fold type is a fundamental precondition in fold recognition. However, the prevailing classification database, such as SCOP (Murzin etal, 1995; Lo Conte etal, 2002) and CATH (Orengo etal, 1997; Pearl etal, 2005), have different classifications (Novotny etal, 2004; Matsuda etal, 2003) and are not constructed for fold recognition (Chen and Crippen, 2006). Thus, it is important to build a protein fold type database with a uniform principle for fold recognition research.Achievements in fold recognition studies overseas have been reported. The fold recognition methods can be classified into the following three categories; based sequence (Dubchak etal, 1995, 1999; Ding and Dubchak, 2001; Shi etal, 2006; Jain etal, 2009), based structure (Marsolo, 2005; Marsolo and Parthasarathy, 2006), and fusion method of sequence and structure (Shi and Zhang, 2009; Shen and Chou, 2009; Gewehr etal, 2007; Ying etal, 2009). The methods are all based on the classification of SCOP, and involve less fold type. The 27 fold types of Ding are widely used. Support Vector Machines (SVM) was used to recognize the 27 fold types by Ding with a best success rate of 56.0% (Ding and Dubchak, 2001). Shi etal used the SVM Fusion Network to predict fold types with an average accuracy of 61.04% (Shi etal, 2006), and they also used the image feature method, got an average accuracy of 71.95% (Shi and Zhang, 2009). Shen and Chou predicted the protein fold pattern with functional domain and sequential evolution information with a success rate of 70.5% for the 27 fold types (Shen and Chou, 2009). Liu etal predicted the protein fold types by the general form of Chou's pseudo amino acid composition for the 27 fold types and obtained better identification results than most of the previous reported results (Liu etal, 2012). Ying etal used a novel data integration approach to enhanced protein fold recognition for the 27 fold types and got a better result, the MKLdiv-dc method improved the fold discrimination accuracy to 75.19% (Ying etal, 2009). These researches play a key role in the method test, involving lesser samples in the database. In general, low accuracy was achieved for the fold recognition, based on small test sets.Recently, some reports about large test sets for fold recognition achieved better results (Jain etal, 2009; Gewehr etal, 2007). Gewehr etal used unique pattern-class mappings to make an automated prediction of SCOP classifications, where in the fold level, the average sensitivity was 93.36% and specificity was 98.13% (Gewehr etal, 2007). Jain etal used supervised machine learning algorithms for protein structure classification based on SCOP classifications and found that the average sensitivities were 0.98, 0.75, 0.90, and 0.97 for the level of class, fold type, superfamily, and family, respectively (Jain etal, 2009).Based on the purpose of build a protein fold type database with a uniform principle for fold recognition research, in earlier studies, a protein fold type database \u2013 LIFCA (low identical protein fold core structures and annotation) with a uniform principle according to the topological connection and spatial arrangement of secondary structure segments (\u03b1-Helix and \u03b2-Sheet) for protein folding type recognition was built (Luo and Li, 2000; Liu etal, 2008; Zhang etal, 2008), and good results have been achieved for the recognition of Globin-like fold (Ren etal, 2007) and another 36 large samples fold (Liu etal, 2009), here we defined the critical number is 4. The average sensitivity, specificity, and Matthew's correlation coefficient (MCC) of the 36 fold types were found to be 90.36%, 99.99% and 0.95, respectively. Results revealed that the HMM can be built for the less sampled fold type using the structure alignment tool together with manual inspection. The larger sample fold types which cannot be built with uniform HMM should be divided into subgroups so that the HMM can be built (Liu etal, 2009).Recently, the functional domain composition method was widely used in bioinformatics, such as subcellular localization (Chou and Cai, 2004a), prediction of peptidase category (Xu etal, 2008), and protein structure class prediction (Chou and Cai, 2004b). Given that one protein or domain can contain one or more functional domains, and in general proteins which belong to the same fold type have similar functions, the functional domain composition method can also be used in fold recognition. In the present study, based on the purpose to test the new functional domain composition method in protein fold recognition, and also to test the classification of LIFCA which built in simple rules, we used this method in the chosen 124 fold types from LIFCA.With the principle of the topology invariance of protein structure\u2019 core, LIFCA database was built according to the topological connection and spatial arrangement of secondary structure segments. It contains 2406 proteins with less than 25% sequence identity, and there are 259 fold types. Compared with the SCOP database, LIFCA merged some SCOP families into one LIFCA fold, and also divided some SCOP families into different LIFCA folds.A total of 124 fold types, which have more than 2 sequence samples were extracted from LIFCA. After removing the proteins without Pfam domain information, a training set with 2240 samples was obtained, covering 827 SCOP families. A total of 38 families were divided based on the LIFCA database.To evaluate the current method and it is sensibility to the samples involving divided SCOP family, the current experimental results were tested from different aspects. A total of 9211 proteins with less than 95% sequence identity from the Astral 1.65 database were chosen as set A. The current method was also evaluated in a test set B, which excludes the duplicated proteins with the training set, and the proteins other than the 124 folds from the test set A, containing 2319 proteins, and 236 proteins involving divided SCOP families."}
